アプリケーション仕様書
1. アプリケーション概要
本アプリケーションは、複数のLLM（大規模言語モデル）を統合し、ユーザーがLLMと対話するためのインターフェースを提供するチャットアプリケーションです。ユーザーは、LLMの種類を選択し、プロンプトを入力することで、LLMからの応答を得ることができます。

2. 機能
LLM選択機能: ユーザーは、利用するLLMの種類（Gemini, Claude, Ollamaなど）を選択できます。
プロンプト入力機能: ユーザーは、LLMに送信するプロンプトを入力できます。
応答表示機能: LLMからの応答を表示します。
ユーザー設定機能: ユーザーごとにLLMの種類やプロンプトテンプレートを設定できます。
ログ出力機能: アプリケーションの動作ログを出力します。
3. アーキテクチャ
本アプリケーションは、以下のコンポーネントで構成されます。

UI (Streamlit): ユーザーインターフェースを提供します。
AIクライアント: 各LLMとの通信を担います (GeminiClient, ClaudeClient, OllamaClient)。
ユーザー設定: ユーザーごとの設定を管理します (UserConfig)。
ログ: アプリケーションの動作ログを記録します (logging)。
4. コンポーネント詳細
4.1. UI (Streamlit)
役割: ユーザーインターフェースの提供
機能:
LLM選択UI
プロンプト入力UI
応答表示UI
設定UI (必要に応じて)
技術: Streamlit
4.2. AIクライアント
役割: 各LLMとの通信
種類:
GeminiClient
ClaudeClient
OllamaClient
機能:
LLMへのプロンプト送信
LLMからの応答受信
技術: 各LLMのAPI (google-generativeai, anthropic, httpx, aiohttp, requests)
4.3. ユーザー設定 (UserConfig)
役割: ユーザーごとの設定管理
機能:
ユーザー設定のロード (JSONファイルから)
LLM種類の取得
プロンプトテンプレートの取得
プロンプトのフォーマット
技術: JSON, ファイルI/O
4.4. ログ (logging)
役割: アプリケーションの動作ログ記録
機能:
ログ出力 (ファイル, コンソール)
ログローテーション
技術: logging (Python標準ライブラリ)
5. データフロー
ユーザーがUIからLLMの種類とプロンプトを入力します。
UIは、選択されたLLMに対応するAIクライアントにプロンプトを送信します。
AIクライアントは、LLMにプロンプトを送信し、応答を受信します。
AIクライアントは、受信した応答をUIに返します。
UIは、受信した応答をユーザーに表示します。
アプリケーションの動作は、ログに記録されます。
6. 環境設定
APIキー: 各LLMのAPIキーが必要です。APIキーは、.env ファイルに設定します。
GOOGLE_API_KEY
ANTHROPIC_API_KEY
GEMINI_API_KEY
Ollama設定: Ollamaを使用する場合、Ollamaのホストとモデルを設定します。
OLLAMA_HOST (例: http://localhost:11434)
OLLAMA_MODEL (例: hf.co/mradermacher/phi-4-deepseek-R1K-RL-EZO-GGUF:Q4_K_S)
ライブラリ: 必要なライブラリは、requirements.txt に記述されています。
streamlit
python-dotenv
google-generativeai
anthropic
httpx
aiohttp
requests
7. ログ
ログファイルは、add_%m%d_%H%M%S.log の形式で生成されます。
ログレベルは、logging.DEBUG に設定されています。
ログローテーションは、最大ファイルサイズ 1MB, バックアップ数 3 に設定されています。
8. ユーザー設定
ユーザー設定は、users ディレクトリに格納された JSON ファイルからロードされます。
各ユーザー設定ファイルには、以下の情報が含まれます。
name: ユーザー名
llm: LLMの種類 (例: "gemini", "claude", "ollama")
personality: プロンプトテンプレート (必要に応じて)
9. 補足事項
UIのデザインは、style.css および style_both.css で定義されています。
view.py には、UIの描画に関する関数が定義されています。
エラー処理は、適切に実装してください。
非同期処理は、asyncio を使用して実装してください。
この仕様書に基づき、他のLLMがアプリケーションを再構築できることを期待します。
